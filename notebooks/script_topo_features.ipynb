{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from rasterio.transform import rowcol\n",
    "import pandas as pd\n",
    "from whitebox import WhiteboxTools\n",
    "from skimage.morphology import skeletonize, remove_small_objects\n",
    "from scipy.ndimage import label\n",
    "from skimage.measure import regionprops\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def array_to_cord(transform, row, col):\n",
    "    \"\"\"\n",
    "    Convert array indices to geographic coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    transform (Affine): The affine transformation object.\n",
    "    row (int): The row index in the array.\n",
    "    col (int): The column index in the array.\n",
    "\n",
    "    Returns:\n",
    "    tuple: The longitude and latitude coordinates.\n",
    "    \"\"\"\n",
    "    lon, lat = transform * (col, row)\n",
    "    return lon, lat\n",
    "\n",
    "\n",
    "def coord_to_array(transform, lon, lat):\n",
    "    \"\"\"\n",
    "    Convert geographic coordinates to array indices.\n",
    "\n",
    "    Parameters:\n",
    "    transform (Affine): The affine transformation object.\n",
    "    lon (float): The longitude coordinate.\n",
    "    lat (float): The latitude coordinate.\n",
    "\n",
    "    Returns:\n",
    "    tuple: The column and row indices in the array.\n",
    "    \"\"\"\n",
    "    col, row = ~transform * (lon, lat)\n",
    "    col, row = int(round(col)), int(round(row))\n",
    "    return col, row\n",
    "\n",
    "\n",
    "def bresenham_line(row0, col0, row1, col1):\n",
    "    \"\"\"\n",
    "    Generate points along a line using Bresenham's algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    row0 (int): The starting row index.\n",
    "    col0 (int): The starting column index.\n",
    "    row1 (int): The ending row index.\n",
    "    col1 (int): The ending column index.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of points (row, col) along the line.\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    drow = abs(row1 - row0)\n",
    "    dcol = abs(col1 - col0)\n",
    "    srow = 1 if row0 < row1 else -1\n",
    "    scol = 1 if col0 < col1 else -1\n",
    "    err = drow - dcol\n",
    "\n",
    "    while True:\n",
    "        points.append((row0, col0))\n",
    "        if row0 == row1 and col0 == col1:\n",
    "            break\n",
    "        e2 = err * 2\n",
    "        if e2 > -dcol:\n",
    "            err -= dcol\n",
    "            row0 += srow\n",
    "        if e2 < drow:\n",
    "            err += drow\n",
    "            col0 += scol\n",
    "\n",
    "    return points\n",
    "\n",
    "\n",
    "def new_bresenham_line(row0, col0, row1, col1, thickness=1):\n",
    "    \"\"\"\n",
    "    Generate points along a thick line using Bresenham's algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    row0 (int): The starting row index.\n",
    "    col0 (int): The starting column index.\n",
    "    row1 (int): The ending row index.\n",
    "    col1 (int): The ending column index.\n",
    "    thickness (int): The thickness of the line.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of points (row, col) along the thick line.\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    drow = abs(row1 - row0)\n",
    "    dcol = abs(col1 - col0)\n",
    "    srow = 1 if row0 < row1 else -1\n",
    "    scol = 1 if col0 < col1 else -1\n",
    "    err = drow - dcol\n",
    "\n",
    "    is_first_point = True  # Flag to avoid adding the contour of the first point\n",
    "\n",
    "    while True:\n",
    "        if is_first_point:\n",
    "            # Add only the central point for the first point\n",
    "            is_first_point = False\n",
    "        else:\n",
    "            # Add neighboring points for subsequent points\n",
    "            for dr in range(-thickness, thickness + 1):\n",
    "                for dc in range(-thickness, thickness + 1):\n",
    "                    points.append((row0 + dr, col0 + dc))\n",
    "\n",
    "        if row0 == row1 and col0 == col1:\n",
    "            break\n",
    "\n",
    "        e2 = err * 2\n",
    "        if e2 > -dcol:\n",
    "            err -= dcol\n",
    "            row0 += srow\n",
    "        if e2 < drow:\n",
    "            err += drow\n",
    "            col0 += scol\n",
    "\n",
    "    return list(set(points))  # Avoid duplicates\n",
    "\n",
    "\n",
    "# List of countries and corresponding date methods\n",
    "dem_countries = [\"Benin\", \"Burkina\", \"Guinee\"]\n",
    "date_methods = [\"datetime\", \"str\", \"datetime\"] #Indicates the type of the column \"DATE\" in the csv\n",
    "\n",
    "# Mapping of countries to inference names i.e. \"Benin.tif\" for DEM and \"BF.csv\" for data points for example\n",
    "dem_to_inference = {\n",
    "    \"Benin\": \"Benin\",\n",
    "    \"Burkina\": \"BF\",\n",
    "    \"Guinee\": \"gui\",\n",
    "    \"Mali\": \"Mali\",\n",
    "    \"Niger\": \"Niger\",\n",
    "    \"Togo\": \"Togo\",\n",
    "}\n",
    "\n",
    "# Main processing loop for each country\n",
    "for training_num in range(len(dem_countries)):\n",
    "    dem_country = dem_countries[training_num]\n",
    "    inference_country = dem_to_inference[dem_country]\n",
    "\n",
    "    # Load training data\n",
    "    training_df = pd.read_excel(f\"Training_data/{inference_country}.xlsx\")\n",
    "\n",
    "    # Filter data based on date method (Meteorological data are available from 2002)\n",
    "    if date_methods[training_num] == \"datetime\":\n",
    "        training_df[\"DATE\"] = pd.to_datetime(training_df[\"DATE\"], errors=\"coerce\")\n",
    "        training_df = training_df[\n",
    "            (training_df[\"DATE\"].dt.year > 2002) & (training_df[\"DATE\"].dt.year < 2025)\n",
    "        ]\n",
    "    elif date_methods[training_num] == \"str\":\n",
    "        training_df = training_df[\n",
    "            training_df[\"DATE\"].apply(lambda row: int(row.split(\"/\")[2])) > 2002\n",
    "        ]\n",
    "    else:\n",
    "        training_df = training_df[training_df[\"DATE\"] > 2002]\n",
    "\n",
    "    # Load DEM data\n",
    "    with rasterio.open(f\"DEM/{dem_country}/{dem_country}.tif\") as src:\n",
    "        transform = src.transform\n",
    "        area = src.read(1)\n",
    "        width = src.width\n",
    "        height = src.height\n",
    "\n",
    "    # Define maximum dimensions and padding to load arrays of reasonable size in RAM\n",
    "    max_width = 5000\n",
    "    max_height = 5000\n",
    "    padding = 100\n",
    "\n",
    "    # Calculate the number of tiles\n",
    "    i_max, j_max = height // max_height + int(\n",
    "        height % max_height != 0\n",
    "    ), width // max_width + int(width % max_width != 0)\n",
    "    print(i_max, j_max)\n",
    "\n",
    "    # Initialize dictionaries for training points and borders\n",
    "    training_points = {}\n",
    "    training_borders = {}\n",
    "\n",
    "    for i in range(i_max):\n",
    "        for j in range(j_max):\n",
    "            row_min = max(0, i * max_height)\n",
    "            col_min = max(0, j * max_width)\n",
    "            row_max = min(height, row_min + max_height)\n",
    "            col_max = min(width, col_min + max_width)\n",
    "            training_borders[row_min, row_max, col_min, col_max] = (i, j)\n",
    "            training_points[i, j] = []\n",
    "\n",
    "    # Assign training points to tiles\n",
    "    coords = zip(training_df.index, training_df[\"LON\"], training_df[\"LAT\"])\n",
    "    for index, lon, lat in coords:\n",
    "        col, row = coord_to_array(transform=transform, lon=lon, lat=lat)\n",
    "        for row_min, row_max, col_min, col_max in training_borders.keys():\n",
    "            if row_min <= row < row_max and col_min <= col < col_max:\n",
    "                training_points[\n",
    "                    training_borders[row_min, row_max, col_min, col_max]\n",
    "                ].append((index, lon, lat))\n",
    "\n",
    "    for key in training_points.keys():\n",
    "        print(f\"Number of points in area {key}: {len(training_points[key])}\")\n",
    "\n",
    "    # Process each tile\n",
    "    for i in range(i_max):\n",
    "        for j in range(j_max):\n",
    "            print(f\"Currently working country: {dem_country} and area: {i, j}\")\n",
    "            if len(training_points[i, j]) == 0: #If no point in this tile\n",
    "                continue\n",
    "            for accumulation_threshold in [1500, 3000]: #Hyperparameter for acccumlation threshold to extract streams\n",
    "                for ridge_size in [30]: #Hyperparameter to identify the ridges (The higher this parameter is, the fewer points are identified as ridges)\n",
    "                    file_path = f\"Topo_features_{dem_country}_{accumulation_threshold}_{ridge_size}_{i}_{j}.csv\"\n",
    "                    if os.path.exists(file_path):\n",
    "                        print(\"The file already exists\")\n",
    "                        continue\n",
    "                    row_min = max(0, i * max_height)\n",
    "                    col_min = max(0, j * max_width)\n",
    "                    row_max = min(height, row_min + max_height)\n",
    "                    col_max = min(width, col_min + max_width)\n",
    "\n",
    "                    # Load DEM data for the tile\n",
    "                    with rasterio.open(f\"DEM/{dem_country}/{dem_country}.tif\") as src:\n",
    "                        transform = src.transform\n",
    "                        window = rasterio.windows.Window( #The window that loads only the tile with a padding around to remove border effects\n",
    "                            max(0, col_min - padding),\n",
    "                            max(0, row_min - padding),\n",
    "                            (col_min - max(0, col_min - padding))\n",
    "                            + col_max\n",
    "                            - col_min\n",
    "                            + (min(width, col_max + padding) - col_max),\n",
    "                            (row_min - max(0, row_min - padding))\n",
    "                            + row_max\n",
    "                            - row_min\n",
    "                            + (min(height, row_max + padding) - row_max),\n",
    "                        )\n",
    "                        dem = src.read(1, window=window)\n",
    "\n",
    "                        results = []\n",
    "\n",
    "                        output_path = f\"Temporary_dem_{dem_country}_{i}_{j}.tif\"\n",
    "\n",
    "                        new_transform = src.window_transform(window)\n",
    "\n",
    "                        new_meta = src.meta.copy()\n",
    "                        new_meta.update(\n",
    "                            {\n",
    "                                \"height\": dem.shape[0],\n",
    "                                \"width\": dem.shape[1],\n",
    "                                \"transform\": new_transform,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                        with rasterio.open(output_path, \"w\", **new_meta) as dst:\n",
    "                            dst.write(dem, 1)\n",
    "\n",
    "                    # Initialize WhiteboxTools\n",
    "                    wbt = WhiteboxTools()\n",
    "\n",
    "                    # Fill depressions\n",
    "                    wbt.fill_depressions(\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_dem_{dem_country}_{i}_{j}.tif\",\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_filled_dem_{dem_country}_{i}_{j}.tif\",\n",
    "                    )\n",
    "\n",
    "                    # Apply Gaussian filter\n",
    "                    wbt.gaussian_filter(\n",
    "                        i=f\"C:/Users/aziza/Desktop/Map/Temporary_filled_dem_{dem_country}_{i}_{j}.tif\",\n",
    "                        output=f\"C:/Users/aziza/Desktop/Map/Temporary_smoothed_dem_{dem_country}_{i}_{j}.tif\",\n",
    "                        sigma=4.0,\n",
    "                    )\n",
    "\n",
    "                    # Calculate flow accumulation\n",
    "                    wbt.d8_flow_accumulation(\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_smoothed_dem_{dem_country}_{i}_{j}.tif\",\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_flow_accum_{dem_country}_{i}_{j}.tif\",\n",
    "                        out_type=\"cells\",\n",
    "                    )\n",
    "\n",
    "                    # Extract streams\n",
    "                    wbt.extract_streams(\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_flow_accum_{dem_country}_{i}_{j}.tif\",\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_streams_{dem_country}_{i}_{j}.tif\",\n",
    "                        threshold=accumulation_threshold,\n",
    "                    )\n",
    "\n",
    "                    # Calculate geomorphons\n",
    "                    wbt.geomorphons(\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_smoothed_dem_{dem_country}_{i}_{j}.tif\",\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_geomorphons_{dem_country}_{i}_{j}.tif\",\n",
    "                    )\n",
    "\n",
    "                    # Calculate slope\n",
    "                    wbt.slope(\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_smoothed_dem_{dem_country}_{i}_{j}.tif\",\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_slope_{dem_country}_{i}_{j}.tif\",\n",
    "                    )\n",
    "\n",
    "                    # Calculate profile curvature\n",
    "                    wbt.profile_curvature(\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_smoothed_dem_{dem_country}_{i}_{j}.tif\",\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_curvature_{dem_country}_{i}_{j}.tif\",\n",
    "                    )\n",
    "\n",
    "                    # Load smoothed DEM and other derived layers\n",
    "                    with rasterio.open(\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_smoothed_dem_{dem_country}_{i}_{j}.tif\"\n",
    "                    ) as src:\n",
    "                        new_transform = src.transform\n",
    "                        new_width = src.width\n",
    "                        new_height = src.height\n",
    "                        smoothed_dem = src.read(1)\n",
    "                        cols, rows = np.meshgrid(\n",
    "                            np.arange(new_width), np.arange(new_height)\n",
    "                        )\n",
    "                        longitudes, latitudes = rasterio.transform.xy(\n",
    "                            new_transform, rows, cols, offset=\"center\"\n",
    "                        )\n",
    "                        longitudes = np.array(longitudes).reshape(smoothed_dem.shape)\n",
    "                        latitudes = np.array(latitudes).reshape(smoothed_dem.shape)\n",
    "\n",
    "                    with rasterio.open(\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_streams_{dem_country}_{i}_{j}.tif\"\n",
    "                    ) as dataset:\n",
    "                        streams = dataset.read(1)\n",
    "\n",
    "                    with rasterio.open(\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_slope_{dem_country}_{i}_{j}.tif\"\n",
    "                    ) as dataset:\n",
    "                        grad = dataset.read(1)\n",
    "\n",
    "                    with rasterio.open(\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_curvature_{dem_country}_{i}_{j}.tif\"\n",
    "                    ) as dataset:\n",
    "                        hessian = dataset.read(1)\n",
    "\n",
    "                    with rasterio.open(\n",
    "                        f\"C:/Users/aziza/Desktop/Map/Temporary_geomorphons_{dem_country}_{i}_{j}.tif\"\n",
    "                    ) as dataset:\n",
    "                        band1 = dataset.read(1)\n",
    "                        band1 = np.maximum(band1, 0)\n",
    "\n",
    "                        mask_black = band1 < 4\n",
    "                        labeled_array, num_features = label(mask_black)\n",
    "\n",
    "                        filtered_mask = np.zeros_like(mask_black)\n",
    "\n",
    "                        for region in regionprops(labeled_array): #Some filtering is applied to correctly identified the ridges\n",
    "                            if region.area >= ridge_size:\n",
    "                                for coord in region.coords:\n",
    "                                    filtered_mask[coord[0], coord[1]] = 1\n",
    "\n",
    "                        ridges = band1 * filtered_mask\n",
    "\n",
    "                        ridges = np.minimum(ridges, 1)\n",
    "\n",
    "                        binary_ridges = ridges > 0\n",
    "\n",
    "                        thin_ridges = skeletonize(binary_ridges)\n",
    "\n",
    "                        ridges = thin_ridges.astype(int)\n",
    "\n",
    "                        thin_ridges_cleaned = remove_small_objects(\n",
    "                            ridges.astype(bool), min_size=2\n",
    "                        )\n",
    "\n",
    "                        ridges = thin_ridges_cleaned.astype(int)\n",
    "\n",
    "                    # Initialize new columns in the training dataframe\n",
    "                    training_df[\"precipitation\"] = 0\n",
    "                    training_df[\"ndvi\"] = 0\n",
    "                    training_df[\"ridge_row\"] = 0\n",
    "                    training_df[\"ridge_col\"] = 0\n",
    "                    training_df[\"stream_row\"] = 0\n",
    "                    training_df[\"stream_col\"] = 0\n",
    "                    training_df[\"alt_stream\"] = 0\n",
    "                    training_df[\"alt_top\"] = 0\n",
    "                    training_df[\"dist_stream\"] = 0\n",
    "                    training_df[\"dist_top\"] = 0\n",
    "                    training_df[\"ratio_alt\"] = 0\n",
    "                    training_df[\"ratio_stream\"] = 0\n",
    "                    training_df[\"ratio_alt\"] = 0\n",
    "                    training_df[\"ratio_dist\"] = 0\n",
    "\n",
    "                    training_df[\"stream_grad_mean\"] = 0\n",
    "                    training_df[\"stream_grad_var\"] = 0\n",
    "                    training_df[\"stream_grad_skew\"] = 0\n",
    "                    training_df[\"stream_grad_kurt\"] = 0\n",
    "                    training_df[\"stream_grad_max\"] = 0\n",
    "                    training_df[\"stream_hessian_mean\"] = 0\n",
    "                    training_df[\"stream_hessian_var\"] = 0\n",
    "                    training_df[\"stream_hessian_skew\"] = 0\n",
    "                    training_df[\"stream_hessian_kurt\"] = 0\n",
    "                    training_df[\"stream_hessian_max\"] = 0\n",
    "\n",
    "                    training_df[\"ridge_grad_mean\"] = 0\n",
    "                    training_df[\"ridge_grad_var\"] = 0\n",
    "                    training_df[\"ridge_grad_skew\"] = 0\n",
    "                    training_df[\"ridge_grad_kurt\"] = 0\n",
    "                    training_df[\"ridge_grad_max\"] = 0\n",
    "                    training_df[\"ridge_hessian_mean\"] = 0\n",
    "                    training_df[\"ridge_hessian_var\"] = 0\n",
    "                    training_df[\"ridge_hessian_skew\"] = 0\n",
    "                    training_df[\"ridge_hessian_kurt\"] = 0\n",
    "                    training_df[\"ridge_hessian_max\"] = 0\n",
    "\n",
    "                    training_df[\"short_grad_mean\"] = 0\n",
    "                    training_df[\"short_grad_var\"] = 0\n",
    "                    training_df[\"short_grad_skew\"] = 0\n",
    "                    training_df[\"short_grad_kurt\"] = 0\n",
    "                    training_df[\"short_grad_max\"] = 0\n",
    "                    training_df[\"short_hessian_mean\"] = 0\n",
    "                    training_df[\"short_hessian_var\"] = 0\n",
    "                    training_df[\"short_hessian_skew\"] = 0\n",
    "                    training_df[\"short_hessian_kurt\"] = 0\n",
    "                    training_df[\"short_hessian_max\"] = 0\n",
    "\n",
    "                    training_df[\"long_grad_mean\"] = 0\n",
    "                    training_df[\"long_grad_var\"] = 0\n",
    "                    training_df[\"long_grad_skew\"] = 0\n",
    "                    training_df[\"long_grad_kurt\"] = 0\n",
    "                    training_df[\"long_grad_max\"] = 0\n",
    "                    training_df[\"long_hessian_mean\"] = 0\n",
    "                    training_df[\"long_hessian_var\"] = 0\n",
    "                    training_df[\"long_hessian_skew\"] = 0\n",
    "                    training_df[\"long_hessian_kurt\"] = 0\n",
    "                    training_df[\"long_hessian_max\"] = 0\n",
    "\n",
    "                    size = 100\n",
    "                    n, p = smoothed_dem.shape\n",
    "\n",
    "                    error_points = set()\n",
    "\n",
    "                    features = []\n",
    "\n",
    "                    # Process each training point\n",
    "                    for index, lon, lat in training_points[i, j]:\n",
    "                        try:\n",
    "                            row_point, col_point = rowcol(new_transform, lon, lat)\n",
    "\n",
    "                            col_mi, row_ma = max(0, col_point - size), min(\n",
    "                                n, row_point + size\n",
    "                            )\n",
    "                            row_mi, col_ma = max(0, row_point - size), min(\n",
    "                                p, col_point + size\n",
    "                            )\n",
    "\n",
    "                            ones_indices = np.argwhere(\n",
    "                                ridges[row_mi:row_ma, col_mi:col_ma] == 1\n",
    "                            )\n",
    "                            distances = np.sqrt(\n",
    "                                (ones_indices[:, 0] - row_point + row_mi) ** 2\n",
    "                                + (ones_indices[:, 1] - col_point + col_mi) ** 2\n",
    "                            )\n",
    "                            sorted_indices = np.argsort(distances)\n",
    "\n",
    "                            ridge_point_row, ridge_point_col = None, None\n",
    "\n",
    "                            for idx in sorted_indices:\n",
    "                                nearest_point = ones_indices[idx]\n",
    "                                candidate_row, candidate_col = (\n",
    "                                    nearest_point[0] + row_mi,\n",
    "                                    nearest_point[1] + col_mi,\n",
    "                                )\n",
    "\n",
    "                                if streams[row_point, col_point] == 1:\n",
    "                                    ridge_point_row, ridge_point_col = (\n",
    "                                        candidate_row,\n",
    "                                        candidate_col,\n",
    "                                    )\n",
    "                                    break\n",
    "\n",
    "                                ridge_points = np.array(\n",
    "                                    [\n",
    "                                        [row, col]\n",
    "                                        for row, col in new_bresenham_line(\n",
    "                                            row0=row_point,\n",
    "                                            col0=col_point,\n",
    "                                            row1=candidate_row,\n",
    "                                            col1=candidate_col,\n",
    "                                        )\n",
    "                                    ]\n",
    "                                )\n",
    "\n",
    "                                # Check if the line crosses a stream point\n",
    "                                if not any(\n",
    "                                    streams[row, col] == 1\n",
    "                                    for row, col in ridge_points[1:]\n",
    "                                ):\n",
    "                                    ridge_point_row, ridge_point_col = (\n",
    "                                        candidate_row,\n",
    "                                        candidate_col,\n",
    "                                    )\n",
    "                                    break\n",
    "\n",
    "                            if ridge_point_row is None or ridge_point_col is None:\n",
    "                                error_points.add(index)\n",
    "                                ridge_point_row, ridge_point_col = 0, 0\n",
    "\n",
    "                            ones_indices = np.argwhere(\n",
    "                                streams[row_mi:row_ma, col_mi:col_ma] == 1\n",
    "                            )\n",
    "                            distances = np.sqrt(\n",
    "                                (ones_indices[:, 0] - row_point + row_mi) ** 2\n",
    "                                + (ones_indices[:, 1] - col_point + col_mi) ** 2\n",
    "                            )\n",
    "                            nearest_index = np.argmin(distances)\n",
    "                            nearest_point = ones_indices[nearest_index]\n",
    "                            stream_point_row, stream_point_col = (\n",
    "                                nearest_point[0] + row_mi,\n",
    "                                nearest_point[1] + col_mi,\n",
    "                            )\n",
    "\n",
    "                            ridge_points = np.array(\n",
    "                                [\n",
    "                                    [row, col]\n",
    "                                    for row, col in bresenham_line(\n",
    "                                        row0=row_point,\n",
    "                                        col0=col_point,\n",
    "                                        row1=candidate_row,\n",
    "                                        col1=candidate_col,\n",
    "                                    )\n",
    "                                ]\n",
    "                            )\n",
    "\n",
    "                            stream_points = np.array(\n",
    "                                [\n",
    "                                    [row, col]\n",
    "                                    for row, col in bresenham_line(\n",
    "                                        row0=row_point,\n",
    "                                        col0=col_point,\n",
    "                                        row1=stream_point_row,\n",
    "                                        col1=stream_point_col,\n",
    "                                    )\n",
    "                                ]\n",
    "                            )\n",
    "\n",
    "                            training_df.at[index, \"ridge_row\"] = ridge_point_row\n",
    "                            training_df.at[index, \"ridge_col\"] = ridge_point_col\n",
    "\n",
    "                            training_df.at[index, \"stream_row\"] = stream_point_row\n",
    "                            training_df.at[index, \"stream_col\"] = stream_point_col\n",
    "\n",
    "                            dem_point = smoothed_dem[row_point, col_point]\n",
    "                            dem_stream = smoothed_dem[\n",
    "                                stream_point_row, stream_point_col\n",
    "                            ]\n",
    "                            dem_ridge = smoothed_dem[ridge_point_row, ridge_point_col]\n",
    "\n",
    "                            training_df.at[index, \"alt_stream\"] = dem_point - dem_stream\n",
    "                            training_df.at[index, \"alt_top\"] = dem_ridge - dem_point\n",
    "\n",
    "                            dist_stream = (\n",
    "                                np.sqrt(\n",
    "                                    (row_point - stream_point_row) ** 2\n",
    "                                    + (col_point - stream_point_col) ** 2\n",
    "                                )\n",
    "                                + 1\n",
    "                            )\n",
    "                            dist_ridge = (\n",
    "                                np.sqrt(\n",
    "                                    (row_point - ridge_point_row) ** 2\n",
    "                                    + (col_point - ridge_point_col) ** 2\n",
    "                                )\n",
    "                                + 1\n",
    "                            )\n",
    "\n",
    "                            training_df.at[index, \"dist_stream\"] = dist_stream\n",
    "                            training_df.at[index, \"dist_top\"] = dist_ridge\n",
    "\n",
    "                            training_df.at[index, \"ratio_alt\"] = (\n",
    "                                dem_point - dem_stream\n",
    "                            ) / (dem_ridge - dem_point + 0.1)\n",
    "                            training_df.at[index, \"ratio_dist\"] = (\n",
    "                                dist_stream / dist_ridge\n",
    "                            )\n",
    "                            training_df.at[index, \"ratio_stream\"] = (\n",
    "                                dem_point - dem_stream\n",
    "                            ) / dist_stream\n",
    "                            training_df.at[index, \"ratio_top\"] = (\n",
    "                                dem_ridge - dem_point\n",
    "                            ) / dist_ridge\n",
    "\n",
    "                            center_row, center_col = row_point, col_point\n",
    "                            max_short_distance = 5\n",
    "                            max_long_distance = 20\n",
    "                            row_mi = max(center_row - max_long_distance, 0)\n",
    "                            row_ma = min(\n",
    "                                center_row + max_long_distance + 2,\n",
    "                                smoothed_dem.shape[0],\n",
    "                            )\n",
    "                            col_mi = max(center_col - max_long_distance, 0)\n",
    "                            col_ma = min(\n",
    "                                center_col + max_long_distance + 2,\n",
    "                                smoothed_dem.shape[1],\n",
    "                            )\n",
    "\n",
    "                            ROWS, COLS = np.meshgrid(\n",
    "                                np.arange(row_mi, row_ma),\n",
    "                                np.arange(col_mi, col_ma),\n",
    "                                indexing=\"ij\",\n",
    "                            )\n",
    "                            distances = np.sqrt(\n",
    "                                (ROWS - center_row) ** 2 + (COLS - center_col) ** 2\n",
    "                            )\n",
    "                            short_distance = np.argwhere(\n",
    "                                distances <= max_short_distance\n",
    "                            ) + np.array([row_mi, col_mi])\n",
    "                            long_distance = np.argwhere(\n",
    "                                distances <= max_long_distance\n",
    "                            ) + np.array([row_mi, col_mi])\n",
    "\n",
    "                            grad_stream = grad[stream_points[:, 0], stream_points[:, 1]]\n",
    "                            hessian_stream = hessian[\n",
    "                                stream_points[:, 0], stream_points[:, 1]\n",
    "                            ]\n",
    "                            training_df.at[index, \"stream_grad_mean\"] = np.mean(\n",
    "                                grad_stream\n",
    "                            )\n",
    "                            training_df.at[index, \"stream_grad_var\"] = np.mean(\n",
    "                                grad_stream\n",
    "                            )\n",
    "                            training_df.at[index, \"stream_grad_skew\"] = stats.skew(\n",
    "                                grad_stream\n",
    "                            )\n",
    "                            training_df.at[index, \"stream_grad_kurt\"] = stats.kurtosis(\n",
    "                                grad_stream\n",
    "                            )\n",
    "                            training_df.at[index, \"stream_grad_max\"] = np.max(\n",
    "                                grad_stream\n",
    "                            )\n",
    "                            training_df.at[index, \"stream_hessian_mean\"] = np.mean(\n",
    "                                hessian_stream\n",
    "                            )\n",
    "                            training_df.at[index, \"stream_hessian_var\"] = np.mean(\n",
    "                                hessian_stream\n",
    "                            )\n",
    "                            training_df.at[index, \"stream_hessian_skew\"] = stats.skew(\n",
    "                                hessian_stream\n",
    "                            )\n",
    "                            training_df.at[\n",
    "                                index, \"stream_hessian_kurt\"\n",
    "                            ] = stats.kurtosis(hessian_stream)\n",
    "                            training_df.at[index, \"stream_hessian_max\"] = np.max(\n",
    "                                hessian_stream\n",
    "                            )\n",
    "\n",
    "                            grad_ridge = grad[ridge_points[:, 0], ridge_points[:, 1]]\n",
    "                            hessian_ridge = hessian[\n",
    "                                ridge_points[:, 0], ridge_points[:, 1]\n",
    "                            ]\n",
    "                            training_df.at[index, \"ridge_grad_mean\"] = np.mean(\n",
    "                                grad_ridge\n",
    "                            )\n",
    "                            training_df.at[index, \"ridge_grad_var\"] = np.mean(\n",
    "                                grad_ridge\n",
    "                            )\n",
    "                            training_df.at[index, \"ridge_grad_skew\"] = stats.skew(\n",
    "                                grad_ridge\n",
    "                            )\n",
    "                            training_df.at[index, \"ridge_grad_kurt\"] = stats.kurtosis(\n",
    "                                grad_ridge\n",
    "                            )\n",
    "                            training_df.at[index, \"ridge_grad_max\"] = np.max(grad_ridge)\n",
    "                            training_df.at[index, \"ridge_hessian_mean\"] = np.mean(\n",
    "                                hessian_ridge\n",
    "                            )\n",
    "                            training_df.at[index, \"ridge_hessian_var\"] = np.mean(\n",
    "                                hessian_ridge\n",
    "                            )\n",
    "                            training_df.at[index, \"ridge_hessian_skew\"] = stats.skew(\n",
    "                                hessian_ridge\n",
    "                            )\n",
    "                            training_df.at[\n",
    "                                index, \"ridge_hessian_kurt\"\n",
    "                            ] = stats.kurtosis(hessian_ridge)\n",
    "                            training_df.at[index, \"ridge_hessian_max\"] = np.max(\n",
    "                                hessian_ridge\n",
    "                            )\n",
    "\n",
    "                            grad_short = grad[\n",
    "                                short_distance[:, 0], short_distance[:, 1]\n",
    "                            ]\n",
    "                            hessian_short = hessian[\n",
    "                                short_distance[:, 0], short_distance[:, 1]\n",
    "                            ]\n",
    "                            training_df.at[index, \"short_grad_mean\"] = np.mean(\n",
    "                                grad_short\n",
    "                            )\n",
    "                            training_df.at[index, \"short_grad_var\"] = np.mean(\n",
    "                                grad_short\n",
    "                            )\n",
    "                            training_df.at[index, \"short_grad_skew\"] = stats.skew(\n",
    "                                grad_short\n",
    "                            )\n",
    "                            training_df.at[index, \"short_grad_kurt\"] = stats.kurtosis(\n",
    "                                grad_short\n",
    "                            )\n",
    "                            training_df.at[index, \"short_grad_max\"] = np.max(grad_short)\n",
    "                            training_df.at[index, \"short_hessian_mean\"] = np.mean(\n",
    "                                hessian_short\n",
    "                            )\n",
    "                            training_df.at[index, \"short_hessian_var\"] = np.mean(\n",
    "                                hessian_short\n",
    "                            )\n",
    "                            training_df.at[index, \"short_hessian_skew\"] = stats.skew(\n",
    "                                hessian_short\n",
    "                            )\n",
    "                            training_df.at[\n",
    "                                index, \"short_hessian_kurt\"\n",
    "                            ] = stats.kurtosis(hessian_short)\n",
    "                            training_df.at[index, \"short_hessian_max\"] = np.max(\n",
    "                                hessian_short\n",
    "                            )\n",
    "\n",
    "                            grad_long = grad[long_distance[:, 0], long_distance[:, 1]]\n",
    "                            hessian_long = hessian[\n",
    "                                long_distance[:, 0], long_distance[:, 1]\n",
    "                            ]\n",
    "                            training_df.at[index, \"long_grad_mean\"] = np.mean(grad_long)\n",
    "                            training_df.at[index, \"long_grad_var\"] = np.mean(grad_long)\n",
    "                            training_df.at[index, \"long_grad_skew\"] = stats.skew(\n",
    "                                grad_long\n",
    "                            )\n",
    "                            training_df.at[index, \"long_grad_kurt\"] = stats.kurtosis(\n",
    "                                grad_long\n",
    "                            )\n",
    "                            training_df.at[index, \"long_grad_max\"] = np.max(grad_long)\n",
    "                            training_df.at[index, \"long_hessian_mean\"] = np.mean(\n",
    "                                hessian_long\n",
    "                            )\n",
    "                            training_df.at[index, \"long_hessian_var\"] = np.mean(\n",
    "                                hessian_long\n",
    "                            )\n",
    "                            training_df.at[index, \"long_hessian_skew\"] = stats.skew(\n",
    "                                hessian_long\n",
    "                            )\n",
    "                            training_df.at[index, \"long_hessian_kurt\"] = stats.kurtosis(\n",
    "                                hessian_long\n",
    "                            )\n",
    "                            training_df.at[index, \"long_hessian_max\"] = np.max(\n",
    "                                hessian_long\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            error_points.add(index)\n",
    "\n",
    "                    # Filter out rows with zero values for ridge_row and ridge_col\n",
    "                    training_df = training_df[\n",
    "                        (training_df[\"ridge_row\"] != 0)\n",
    "                        | (training_df[\"ridge_row\"] != 0)\n",
    "                    ]\n",
    "\n",
    "                    # Save the processed data to a CSV file\n",
    "                    training_df.to_csv(\n",
    "                        f\"Topo_features_{dem_country}_{accumulation_threshold}_{ridge_size}_{i}_{j}.csv\"\n",
    "                    )\n",
    "\n",
    "                    # Remove temporary files\n",
    "                    os.remove(f\"Temporary_dem_{dem_country}_{i}_{j}.tif\")\n",
    "                    os.remove(f\"Temporary_filled_dem_{dem_country}_{i}_{j}.tif\")\n",
    "                    os.remove(f\"Temporary_smoothed_dem_{dem_country}_{i}_{j}.tif\")\n",
    "                    os.remove(f\"Temporary_flow_accum_{dem_country}_{i}_{j}.tif\")\n",
    "                    os.remove(f\"Temporary_streams_{dem_country}_{i}_{j}.tif\")\n",
    "                    os.remove(f\"Temporary_geomorphons_{dem_country}_{i}_{j}.tif\")\n",
    "                    os.remove(f\"Temporary_slope_{dem_country}_{i}_{j}.tif\")\n",
    "                    os.remove(f\"Temporary_curvature_{dem_country}_{i}_{j}.tif\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
